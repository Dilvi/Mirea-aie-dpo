# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):

- `S07-hw-dataset-01.csv`
- `S07-hw-dataset-02.csv`
- `S07-hw-dataset-04.csv`

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000, 9)
- Признаки: числовые (8 float-признаков `f01..f08`) + `sample_id` (ID, не используется как признак)
- Пропуски: нет
- "Подлости" датасета: признаки в разных шкалах + шумовые/разномасштабные признаки → без `StandardScaler` distance-based методы будут смещаться в сторону признаков с большой дисперсией.

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000, 4)
- Признаки: числовые (`x1`, `x2`, `z_noise`) + `sample_id` (ID)
- Пропуски: нет
- "Подлости" датасета: нелинейная структура + шумовой признак `z_noise`. KMeans может давать умеренное качество, DBSCAN сильно зависит от `eps`.

### 1.3 Dataset C

- Файл: `S07-hw-dataset-04.csv`
- Размер: (10000, 33)
- Признаки: числовые (30 float `n01..n30`) + 2 категориальных (`cat_a`, `cat_b`) + `sample_id` (ID)
- Пропуски: есть в части числовых колонок (максимальные доли около ~2%)
- "Подлости" датасета: высокая размерность + категориальные признаки + пропуски. Для корректного сравнения кластеризаторов нужен явный препроцессинг (imputation + scaling + encoding). DBSCAN на высокой размерности часто деградирует (почти всё становится шумом).

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол.

- Препроцессинг:
  - Числовые признаки: `SimpleImputer(strategy="median")` + `StandardScaler()`
  - Категориальные (только Dataset C): `SimpleImputer(strategy="most_frequent")` + `OneHotEncoder(handle_unknown="ignore")`
  - Для каждого датасета один и тот же препроцессинг применялся одинаково ко всем сравниваемым алгоритмам.

- Поиск гиперпараметров:
  - KMeans: перебор `k` в диапазоне 2…6 (фиксировались `random_state`, `n_init`)
  - DBSCAN: перебор `eps` по сетке (в экспериментах использовались значения из разумного диапазона), `min_samples=5`
  - Критерий выбора лучшего решения: `max_silhouette` (как основной критерий). Для DBSCAN метрики считались по non-noise точкам (шум `label=-1` исключался из расчёта метрик), дополнительно анализировалась доля шума.

- Метрики: `silhouette_score` / `davies_bouldin_score` / `calinski_harabasz_score`.
  - Для DBSCAN: отдельно выводилась `noise_share`. Метрики считались по non-noise точкам (это позволяет сравнивать “качество кластеров” отдельно от доли выбросов/шума).

- Визуализация:
  - PCA(2D) scatter для лучшего решения на каждом датасете
  - Графики подбора параметров/метрик (например, silhouette vs k или silhouette vs eps)
  - t-SNE не использовался (опционально).

## 3. Models

Перечислите, какие модели сравнивали **на каждом датасете**, и какие параметры подбирали.

Минимум (для каждого датасета):

- KMeans:
  - поиск `k` (2…6),
  - фиксировались `random_state`, `n_init`,
  - оценка по silhouette/DB/CH.
- DBSCAN:
  - подбор `eps` и `min_samples`,
  - анализ `noise_share`,
  - метрики считались по non-noise точкам.

(Дополнительные алгоритмы, например AgglomerativeClustering, в этой работе не использовались.)

## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A

- Лучший метод и параметры: **KMeans**, `k=2` (критерий: max silhouette)
- Метрики (silhouette / DB / CH):
  - silhouette = **0.521640**
  - davies_bouldin = **0.685330**
  - calinski_harabasz = **11786.954623**
- Если был DBSCAN: лучшая конфигурация из сравнения давала сопоставимые метрики, но финально выбран KMeans по критерию `max_silhouette` (и как более “простая”/интерпретируемая модель на этом датасете).
- Коротко: структура данных хорошо соответствует геометрии KMeans после масштабирования (высокий silhouette и низкий DB).

### 4.2 Dataset B

- Лучший метод и параметры: **KMeans**, `k=2` (критерий: max silhouette)
- Метрики (silhouette / DB / CH):
  - silhouette = **0.306861**
  - davies_bouldin = **1.323472**
  - calinski_harabasz = **3573.393333**
- Если был DBSCAN: лучшая конфигурация DBSCAN (`eps=0.6`, `min_samples=5`) имела:
  - silhouette = 0.138189, DB = 0.901970, CH = 69.331686,
  - доля шума = **0.022625**, число кластеров = 12,
  но по основному критерию silhouette победил KMeans.
- Коротко: датасет сложнее (нелинейность + шумовой признак), поэтому качество у KMeans умеренное, но всё равно лучше среди протестированных настроек.

### 4.3 Dataset C

- Лучший метод и параметры: **DBSCAN**, `eps=1.0`, `min_samples=5` (критерий: max silhouette; метрики DBSCAN считались по non-noise)
- Метрики (silhouette / DB / CH):
  - silhouette = **0.493620**
  - davies_bouldin = **0.701813**
  - calinski_harabasz = **15.385195**
- Если был DBSCAN: доля шума **0.999** (99.9%), число кластеров = 2.
- Коротко: по формальному критерию silhouette (на non-noise) DBSCAN победил, но это решение сопровождается почти полной “шумовой” разметкой. Поэтому в интерпретации важно явно учитывать trade-off: DBSCAN выделил очень маленькие плотные группы, а подавляющее большинство точек посчитал шумом. Для практического применения такой результат может быть неудачным, но как демонстрация поведения DBSCAN на высокой размерности — показателен.

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- Где KMeans "ломается" и почему?
  - На данных с нелинейной геометрией и шумом (Dataset B) silhouette заметно ниже, т.к. KMeans предполагает “шарообразные” кластеры и чувствителен к шумовым признакам.
  - На высокоразмерных данных (Dataset C) KMeans может работать, но качество и интерпретация сильно зависят от препроцессинга и представления категориальных признаков.

- Где DBSCAN выигрывает и почему?
  - DBSCAN может выделять плотные группы и отбрасывать выбросы как шум. На Dataset C это проявилось максимально: модель нашла 2 плотных кластера, но почти всё остальное стало шумом (noise_share≈0.999).

- Что сильнее всего влияло на результат?
  - Масштабирование (Dataset A) — без него результат KMeans/DBSCAN и метрики сильно меняются.
  - Наличие шумового признака (Dataset B).
  - Пропуски + категориальные + высокая размерность (Dataset C) — без imputation/OHE сравнение алгоритмов некорректно, а DBSCAN становится крайне чувствительным.

### 5.2 Устойчивость (обязательно для одного датасета)

- Проверка устойчивости: 5 запусков KMeans на Dataset A (k=2) с разными `random_state`, сравнение разбиений через ARI.
- Результат: ARI между разбиениями = **1.0**, средний ARI = **1.0**.
- Вывод: решение **устойчивое** — разбиение полностью совпадает между запусками, структура кластеров выраженная.

### 5.3 Интерпретация кластеров

- Интерпретация выполнялась через анализ профилей признаков по кластерам (агрегации вроде средних/медиан) и визуальную диагностику PCA(2D).
- Выводы:
  - Dataset A: кластеры хорошо разделяются и устойчивы; различия проявляются по комбинациям числовых признаков после scaling.
  - Dataset B: кластеры выделяются хуже из-за нелинейности и шума; `z_noise` снижает разделимость.
  - Dataset C: DBSCAN выделил крайне “узкие” плотные группы и отнёс почти всё к шуму — это интерпретируется как признак того, что в высоком измерении плотностной критерий становится слишком строгим для большинства точек.

## 6. Conclusion

- Масштабирование критично для distance-based методов, иначе признаки с большой шкалой доминируют.
- KMeans показывает лучший и самый устойчивый результат на датасете с близкой к “шарообразной” структурой (Dataset A).
- На датасетах с нелинейностью и шумом качество KMeans падает (Dataset B), и внутренние метрики помогают это увидеть.
- DBSCAN способен выделять плотные группы и шум, но на высокой размерности может “переусердствовать” и превращать почти всё в шум (Dataset C).
- Внутренние метрики (silhouette/DB/CH) полезны, но их нужно читать вместе с числом кластеров и долей шума, особенно для DBSCAN.
- PCA(2D) — хорошая визуальная диагностика, но это проекция: её нужно использовать как иллюстрацию, а не как доказательство качества.
